{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9e4b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8e043b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193422\n",
      "64474\n"
     ]
    }
   ],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, file_name):\n",
    "        # read csv file and load row data into input and output\n",
    "        raw_data = pd.read_csv(file_name)\n",
    "        inputX = raw_data.iloc[0:len(raw_data), 0:15].values\n",
    "        outputY = raw_data.iloc[0:len(raw_data), 15].values\n",
    "        \n",
    "        #convert to torch tensors\n",
    "        self.data_X = torch.tensor(inputX,dtype=torch.float32)\n",
    "        self.data_Y = torch.tensor(outputY)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_Y)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data_X[idx], self.data_Y[idx]\n",
    "    \n",
    "dataSet = FeatureDataset(\"F:\\m100_for_model.csv\")\n",
    "\n",
    "lenTrainingPercen = 75\n",
    "lenTestPercen = 25\n",
    "numTraining = math.ceil((len(dataSet)/100)*lenTrainingPercen)\n",
    "numTest = len(dataSet)-numTraining\n",
    "print(numTraining)\n",
    "print(numTest)\n",
    "train, val = torch.utils.data.random_split(dataSet, [numTraining, numTest])  # spilt the data to training set as well as test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfba11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ad0a6825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "# DATA = pd.read_csv(\"F:\\m100_for_model.csv\")\n",
    "# k = DATA.iloc[0:len(DATA),15]\n",
    "# k\n",
    "# DATA_pureInput = DATA.drop(['energy','aircraft'], axis = 1).values\n",
    "# Input_tensor = torch.tensor(DATA_pureInput)\n",
    "# DATA_output = DATA['energy'].values\n",
    "# Label_tensor = torch.tensor(DATA_output)\n",
    "# Label_tensor\n",
    "# x,y = torch.utils.data.random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
    "# print(x.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3b43cfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train,\n",
    "                                           batch_size = 2048, \n",
    "                                           shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val,\n",
    "                                         batch_size = 2048, \n",
    "                                         shuffle = True) \n",
    "examples = enumerate(train_loader) \n",
    "batch_idx, (example_data, example_targets) = next(examples)  # X = example_data, Y = example_targets\n",
    "print(batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "16e94611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ownMLP(\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=15, out_features=44, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (28): ReLU(inplace=True)\n",
       "    (29): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (30): ReLU(inplace=True)\n",
       "    (31): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (34): ReLU(inplace=True)\n",
       "    (35): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (40): ReLU(inplace=True)\n",
       "    (41): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (44): ReLU(inplace=True)\n",
       "    (45): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (46): ReLU(inplace=True)\n",
       "    (47): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (50): ReLU(inplace=True)\n",
       "    (51): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (52): ReLU(inplace=True)\n",
       "    (53): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (54): ReLU(inplace=True)\n",
       "    (55): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (56): ReLU(inplace=True)\n",
       "    (57): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (58): ReLU(inplace=True)\n",
       "    (59): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (60): ReLU(inplace=True)\n",
       "    (61): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (62): ReLU(inplace=True)\n",
       "    (63): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (64): ReLU(inplace=True)\n",
       "    (65): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (66): ReLU(inplace=True)\n",
       "    (67): Linear(in_features=44, out_features=44, bias=True)\n",
       "    (68): ReLU(inplace=True)\n",
       "    (69): Linear(in_features=44, out_features=1, bias=True)\n",
       "    (70): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ownMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ownMLP, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim,44), #1, # 1st para is input size after flatten, output is the number of neurons in hidden layers after input layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 4\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 5\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 6\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 7\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 8\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 9\n",
    "            nn.ReLU(inplace=True),   \n",
    "            nn.Linear(44,44), # 10\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 11\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 12\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 13\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 14\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 15\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 17\n",
    "            nn.ReLU(inplace=True),    \n",
    "            nn.Linear(44,44), # 18\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 19\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 20\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 21\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 22\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 23\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 24\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 25\n",
    "            nn.ReLU(inplace=True),   \n",
    "            nn.Linear(44,44), # 26\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 27\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 29\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 30\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 31\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 32\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,44), # 33\n",
    "            nn.ReLU(inplace=True),        \n",
    "            nn.Linear(44,44), # 34\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(44,output_dim), # 35\n",
    "            nn.ReLU(inplace=True),             \n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = ownMLP(15,1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f6c5708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 450 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [144]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inData)\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(outData\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 20\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#         # Backward and optimize, # another word for optimize is also called move the next step/going to next best position\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1047\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, Tensor)\n\u001b[1;32m-> 1047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\functional.py:2693\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2692\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\functional.py:2388\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2385\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected input batch_size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) to match target batch_size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m   2386\u001b[0m     )\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 2388\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2389\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m   2390\u001b[0m     ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mnll_loss2d(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 450 is out of bounds."
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "Loss = []\n",
    "Acc = []\n",
    "Val_Loss = []\n",
    "Val_Acc = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    acc = 0\n",
    "    val_acc = 0\n",
    "    for i, (inputData, outputData) in enumerate(train_loader):  # sweep through the entire training set\n",
    "        model.train()\n",
    "        inData = inputData.to(device)\n",
    "        outData = outputData.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # empty the previous gradient\n",
    "        # Forward pass\n",
    "        outputs = model(inData)\n",
    "        print(outData.long().shape)\n",
    "        \n",
    "        loss = criterion(outputs, outData.long())\n",
    "#         # Backward and optimize, # another word for optimize is also called move the next step/going to next best position\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Checking accuracy\n",
    "        preds = outputs.data.max(dim=1,keepdim=True)[1]\n",
    "        acc += preds.eq(outData.data.view_as(preds)).cpu().sum()\n",
    "    acc = acc/len(train_loader.dataset) * 100\n",
    "    \n",
    "    # After go through entire training set, now we can use the knowledge we learning, to practice with the test set\n",
    "    for i, (inputData, outputData) in enumerate(val_loader):\n",
    "        model.eval()\n",
    "        inputData = inputData.to(device)\n",
    "        outputData = outputData.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputData)\n",
    "        val_loss = criterion(outputs, outputData)\n",
    "        # Checking accuracy\n",
    "        preds = outputs.data.max(dim=1,keepdim=True)[1]\n",
    "        val_acc += preds.eq(outputData.data.view_as(preds)).cpu().sum()\n",
    "    val_acc = val_acc/len(val_loader.dataset) * 100\n",
    "    print(\"Epoch {} =>  loss : {loss:.2f};   Accuracy : {acc:.2f}%;   Val_loss : {val_loss:.2f};   Val_Accuracy : {val_acc:.2f}%\".format(epoch+1, loss=loss.item(), acc=acc, val_loss=val_loss.item(), val_acc=val_acc))\n",
    "    Loss.append(loss)\n",
    "    Acc.append(acc)\n",
    "    Val_Loss.append(val_loss)\n",
    "    Val_Acc.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08913c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e37836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
